import os
import sys
import torch
import argparse
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
from peft import PeftModel
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# Fix encoding cho Windows
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

def create_rag_system(model_base_id, adapter_path, dataset_name, vectorstore_path, embedding_model_name="sentence-transformers/all-MiniLM-L6-v2", chunk_size=512, chunk_overlap=50, k_retrieval=3):
    """
    Táº¡o há»‡ thá»‘ng RAG vá»›i fine-tuned model, embeddings vÃ  vectorstore.
    Náº¿u vectorstore Ä‘Ã£ tá»“n táº¡i, sáº½ load láº¡i thay vÃ¬ táº¡o má»›i.
    
    Args:
        model_base_id: ID cá»§a base model (vÃ­ dá»¥: "Qwen/Qwen2.5-1.5B-Instruct")
        adapter_path: ÄÆ°á»ng dáº«n tá»›i adapter Ä‘Ã£ fine-tuned
        dataset_name: TÃªn dataset trÃªn Hugging Face (vÃ­ dá»¥: "ruslanmv/ai-medical-chatbot")
        vectorstore_path: ÄÆ°á»ng dáº«n lÆ°u/load vectorstore
        embedding_model_name: TÃªn model embedding
        chunk_size: KÃ­ch thÆ°á»›c chunk
        chunk_overlap: Äá»™ trÃ¹ng láº·p giá»¯a cÃ¡c chunks
        k_retrieval: Sá»‘ lÆ°á»£ng documents Ä‘á»ƒ retrieve
    
    Returns:
        chain: RAG chain
        vectorstore: FAISS vectorstore
    """
    print("--- Äang khá»Ÿi táº¡o há»‡ thá»‘ng RAG ---")
    
    # Kiá»ƒm tra GPU
    print("\n" + "="*60)
    print("ğŸ” KIá»‚M TRA Há»† THá»NG")
    print("="*60)
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ… CUDA version: {torch.version.cuda}")
        print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
        print(f"âœ… VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        print(f"âœ… Sá»­ dá»¥ng GPU Ä‘á»ƒ tÃ­nh toÃ¡n")
    else:
        device = torch.device("cpu")
        print("âš ï¸ CUDA khÃ´ng kháº£ dá»¥ng, sáº½ sá»­ dá»¥ng CPU")
    
    print("="*60 + "\n")
    
    # 1. Load fine-tuned model vÃ  táº¡o pipeline
    print(f"--- Äang táº£i base model: {model_base_id} ---")
    tokenizer = AutoTokenizer.from_pretrained(model_base_id, trust_remote_code=True)
    
    # Cáº¥u hÃ¬nh 4-bit quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )
    
    base_model = AutoModelForCausalLM.from_pretrained(
        model_base_id,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        dtype=torch.float16
    )
    
    print(f"--- Äang táº£i adapter tá»«: {adapter_path} ---")
    finetuned_model = PeftModel.from_pretrained(base_model, adapter_path)
    
    # Táº¡o Hugging Face Pipeline
    print("--- Äang táº¡o pipeline ---")
    from transformers import pipeline
    pipe = pipeline(
        "text-generation",
        model=finetuned_model,
        tokenizer=tokenizer,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.1,
        top_p=0.9,
        repetition_penalty=1.2,
        no_repeat_ngram_size=3,
        return_full_text=False,
        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id else tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id
    )
    
    # Bá»c pipeline thÃ nh LangChain LLM
    llm = HuggingFacePipeline(pipeline=pipe)
    print("âœ… LLM Fine-Tuned Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o.")
    
    # 2. Load hoáº·c táº¡o vectorstore
    embedding_model_name = embedding_model_name or "sentence-transformers/all-MiniLM-L6-v2"
    print(f"--- Äang táº£i embedding model: {embedding_model_name} ---")
    embeddings = HuggingFaceBgeEmbeddings(model_name=embedding_model_name)
    print(f"âœ… ÄÃ£ táº£i embedding model.")
    
    # Kiá»ƒm tra xem vectorstore Ä‘Ã£ tá»“n táº¡i chÆ°a
    index_path = os.path.join(vectorstore_path, "index.faiss")
    if os.path.exists(index_path):
        print(f"--- Vectorstore Ä‘Ã£ tá»“n táº¡i, Ä‘ang load tá»«: {vectorstore_path} ---")
        vectorstore = FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… Vectorstore Ä‘Ã£ Ä‘Æ°á»£c load thÃ nh cÃ´ng.")
    else:
        print(f"--- Vectorstore chÆ°a tá»“n táº¡i, Ä‘ang táº¡o má»›i ---")
        # Load dataset vÃ  táº¡o documents
        print(f"--- Äang táº£i dataset: {dataset_name} ---")
        subset_size = 1000
        dataset = load_dataset(dataset_name, split=f'train[:{subset_size}]')
        
        rag_documents = []
        for sample in dataset:
            content = (
                f"CÃ¢u há»i bá»‡nh nhÃ¢n: {sample['Patient']}\n"
                f"TÃ¬nh tráº¡ng/MÃ´ táº£: {sample['Description']}\n"
                f"CÃ¢u tráº£ lá»i chuyÃªn mÃ´n: {sample['Doctor']}"
            )
            metadata = {"source": "ai-medical-chatbot-dataset"}
            rag_documents.append(Document(page_content=content, metadata=metadata))
        
        print(f"âœ… ÄÃ£ táº£i {len(rag_documents)} máº«u tá»« dataset.")
        
        # Chia nhá» documents (Text Splitting)
        print("--- Äang chia nhá» documents ---")
        text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        chunks_of_text = text_splitter.split_documents(rag_documents)
        print(f"âœ… ÄÃ£ chia thÃ nh {len(chunks_of_text)} chunks.")
        
        # Táº¡o vectorstore
        print("--- Äang táº¡o vectorstore vÃ  indexing ---")
        vectorstore = FAISS.from_documents(chunks_of_text, embeddings)
        
        # LÆ°u vectorstore
        os.makedirs(os.path.dirname(vectorstore_path) if os.path.dirname(vectorstore_path) else ".", exist_ok=True)
        vectorstore.save_local(vectorstore_path)
        print(f"âœ… Vectorstore Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {vectorstore_path}")
    
    # 3. Khá»Ÿi táº¡o Retriever
    retriever = vectorstore.as_retriever(search_kwargs={"k": k_retrieval})
    print(f"âœ… Retriever Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o (k={k_retrieval}).")
    
    # 4. Táº¡o prompt template
    template = """Answer the question based on the following context:

{context}

Question: {question}
"""
    prompt = ChatPromptTemplate.from_template(template)
    
    # 5. Táº¡o RAG chain
    def format_docs(docs):
        return "\n\n".join([d.page_content for d in docs])
    
    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    print("âœ… Há»‡ thá»‘ng RAG Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o hoÃ n táº¥t!")
    
    return chain, vectorstore

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Thiáº¿t láº­p há»‡ thá»‘ng RAG vá»›i fine-tuned model")
    parser.add_argument("--model_base_id", type=str, default="Qwen/Qwen2.5-0.5B-Instruct", 
                        help="ID cá»§a base model")
    parser.add_argument("--adapter_path", type=str, default="models/qwen-medical-finetuned/final_adapter",
                        help="ÄÆ°á»ng dáº«n tá»›i adapter Ä‘Ã£ fine-tuned")
    parser.add_argument("--dataset_name", type=str, default="ruslanmv/ai-medical-chatbot",
                        help="TÃªn dataset trÃªn Hugging Face")
    parser.add_argument("--vectorstore_path", type=str, default="data/rag_vectorstore",
                        help="ÄÆ°á»ng dáº«n lÆ°u vectorstore")
    parser.add_argument("--embedding_model", type=str, default="sentence-transformers/all-MiniLM-L6-v2",
                        help="TÃªn model embedding")
    parser.add_argument("--chunk_size", type=int, default=512,
                        help="KÃ­ch thÆ°á»›c chunk")
    parser.add_argument("--chunk_overlap", type=int, default=50,
                        help="Äá»™ trÃ¹ng láº·p giá»¯a cÃ¡c chunks")
    parser.add_argument("--k_retrieval", type=int, default=3,
                        help="Sá»‘ lÆ°á»£ng documents Ä‘á»ƒ retrieve")
    
    args = parser.parse_args()
    
    chain, vectorstore = create_rag_system(
        model_base_id=args.model_base_id,
        adapter_path=args.adapter_path,
        dataset_name=args.dataset_name,
        vectorstore_path=args.vectorstore_path,
        embedding_model_name=args.embedding_model,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        k_retrieval=args.k_retrieval
    )
    
    # Test query
    print("\n--- TEST QUERY ---")
    query = "TÃ´i bá»‹ tÃª vÃ  ngá»©a ran á»Ÿ tay vÃ o ban Ä‘Ãªm, nguyÃªn nhÃ¢n lÃ  gÃ¬ vÃ  cÃ¡ch Ä‘iá»u trá»‹ thÃ´ng thÆ°á»ng lÃ  gÃ¬?"
    print(f"Query: {query}\n")
    response = chain.invoke(query)
    print(f"Response: {response}")

