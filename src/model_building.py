import os
import sys
import torch
import argparse
from datasets import load_from_disk
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer, 
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
# Import Class DataCollator t·ª´ b∆∞·ªõc processing
from data_processing import DataCollatorForCompletionOnlyLM

# Fix encoding cho Windows
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

def build_and_train(input_dir, output_dir, model_id):
    print(f"--- ƒêang kh·ªüi t·∫°o m√¥ h√¨nh: {model_id} ---")
    
    # 1. C·∫•u h√¨nh Quantization (4-bit)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    # 2. Load Tokenizer & Model
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        use_cache=False
    )

    # 3. Chu·∫©n b·ªã model cho QLoRA
    model.gradient_checkpointing_enable()
    model = prepare_model_for_kbit_training(model)

    # 4. Thi·∫øt l·∫≠p c·∫•u h√¨nh LoRA
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # 5. Load d·ªØ li·ªáu ƒë√£ tokenize
    print(f"--- ƒêang n·∫°p d·ªØ li·ªáu t·ª´: {input_dir} ---")
    tokenized_train = load_from_disk(os.path.join(input_dir, "train"))
    tokenized_val = load_from_disk(os.path.join(input_dir, "val"))

    # 6. Kh·ªüi t·∫°o Data Collator
    data_collator = DataCollatorForCompletionOnlyLM(tokenizer=tokenizer)

    # 7. Thi·∫øt l·∫≠p tham s·ªë hu·∫•n luy·ªán (Ch·∫ø ƒë·ªô ch·∫°y th·ª≠ 0.1 epoch)
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=1,                # Gi·∫£m xu·ªëng 1 epoch nh∆∞ b·∫°n mu·ªën
        per_device_train_batch_size=1,     # B·∫Øt bu·ªôc l√† 1
        gradient_accumulation_steps=32,    # TƒÉng l√™n 32 ƒë·ªÉ √≠t ph·∫£i ghi d·ªØ li·ªáu v√†o RAM h∆°n
        learning_rate=2e-4,
        logging_steps=5,                   # Gi·∫£m t·∫ßn su·∫•t in log
        eval_strategy="no",                # T·∫Øt ho√†n to√†n ƒë·ªÉ ti·∫øt ki·ªám RAM/VRAM
        save_strategy="no",                # Kh√¥ng l∆∞u checkpoint gi·ªØa ch·ª´ng, ch·ªâ l∆∞u c√°i cu·ªëi
        fp16=True,
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False},
        dataloader_num_workers=0,          # Kh√¥ng d√πng ti·∫øn tr√¨nh ph·ª•
        report_to="none"
    )

    # 8. Ch·∫°y Trainer (ƒê√£ b·ªè callbacks)
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        data_collator=data_collator
    )

    print("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh hu·∫•n luy·ªán...")
    trainer.train()

    # 9. L∆∞u Adapter
    final_path = os.path.join(output_dir, "final_adapter")
    trainer.save_model(final_path)
    print(f"‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t! Adapter ƒë∆∞·ª£c l∆∞u t·∫°i: {final_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_dir", type=str, default="data/tokenized")
    parser.add_argument("--output_dir", type=str, default="models/qwen-medical-finetuned")
    parser.add_argument("--model_id", type=str, default="Qwen/Qwen2.5-0.5B-Instruct")
    args = parser.parse_args()
    
    build_and_train(args.input_dir, args.output_dir, args.model_id)