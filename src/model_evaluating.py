import os
import sys
import json
import torch
import argparse
import io
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import evaluate

# Import h√†m create RAG system
from model_RAG import create_rag_system

# Fix l·ªói hi·ªÉn th·ªã k√Ω t·ª± ƒë·∫∑c bi·ªát tr√™n Windows Terminal
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stdout.reconfigure(encoding='utf-8')

SYSTEM_PROMPT = "You are a professional and detailed medical assistant, providing information based on scientific evidence."

def extract_assistant_response(output_ids, inputs, tokenizer):
    """Extract the clean assistant response from the model's output IDs."""
    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)
    assistant_prefix = "<|im_start|>assistant\n"
    
    start_index = generated_text.find(assistant_prefix)
    if start_index != -1:
        response_with_end_tokens = generated_text[start_index + len(assistant_prefix):]
        response = response_with_end_tokens.split("<|im_end|>")[0].strip()
    else:
        input_len = inputs['input_ids'].shape[1]
        response = tokenizer.decode(output_ids[0, input_len:], skip_special_tokens=True).strip()
    return response

def generate_response_base_finetuned(model, tokenizer, question, max_new_tokens=256):
    """Generate response t·ª´ base model ho·∫∑c fine-tuned model"""
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": question}
    ]
    
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    
    return extract_assistant_response(output_ids, inputs, tokenizer)

def run_inference_base_finetuned(model, tokenizer, samples):
    """H√†m b·ªï tr·ª£ ƒë·ªÉ ch·∫°y d·ª± ƒëo√°n h√†ng lo·∫°t cho base/finetuned model"""
    predictions = []
    references = []
    
    for item in tqdm(samples, desc="D·ª± ƒëo√°n", leave=False):
        full_text = item['text']
        parts = full_text.split("<|im_start|>assistant\n")
        if len(parts) < 2:
            continue
        prompt = parts[0] + "<|im_start|>assistant\n"
        reference = parts[1].replace("<|im_end|>", "").strip()
        
        # Extract question t·ª´ prompt
        user_part = prompt.split("<|im_start|>user\n")[1].split("<|im_end|>")[0].strip()
        
        pred = generate_response_base_finetuned(model, tokenizer, user_part)
        predictions.append(pred)
        references.append(reference)
    
    return predictions, references

def run_inference_rag(chain, samples):
    """H√†m b·ªï tr·ª£ ƒë·ªÉ ch·∫°y d·ª± ƒëo√°n h√†ng lo·∫°t cho RAG model"""
    predictions = []
    references = []
    
    for item in tqdm(samples, desc="D·ª± ƒëo√°n RAG", leave=False):
        full_text = item['text']
        parts = full_text.split("<|im_start|>assistant\n")
        if len(parts) < 2:
            continue
        reference = parts[1].replace("<|im_end|>", "").strip()
        
        # Extract question t·ª´ prompt
        user_part = full_text.split("<|im_start|>user\n")[1].split("<|im_end|>")[0].strip()
        
        try:
            pred = chain.invoke(user_part)
            predictions.append(pred)
            references.append(reference)
        except Exception as e:
            print(f"Error in RAG inference: {e}")
            predictions.append("")
            references.append(reference)
    
    return predictions, references

def evaluate_and_compare(val_file, output_metrics, model_id, adapter_path, vectorstore_path):
    # Ki·ªÉm tra v√† ƒë·∫£m b·∫£o s·ª≠ d·ª•ng GPU
    print("\n" + "="*60)
    print("üîç KI·ªÇM TRA H·ªÜ TH·ªêNG")
    print("="*60)
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"‚úÖ CUDA version: {torch.version.cuda}")
        print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
        print(f"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        print(f"‚úÖ S·ª≠ d·ª•ng GPU ƒë·ªÉ t√≠nh to√°n")
    else:
        device = torch.device("cpu")
        print("‚ö†Ô∏è CUDA kh√¥ng kh·∫£ d·ª•ng, s·∫Ω s·ª≠ d·ª•ng CPU")
    
    print("="*60 + "\n")
    
    # 1. C·∫•u h√¨nh Quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )
    
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # 2. ƒê·ªçc d·ªØ li·ªáu validation
    with open(val_file, 'r', encoding='utf-8') as f:
        val_data = [json.loads(line) for line in f]
    samples = val_data[:5]  # L·∫•y 50 m·∫´u ƒë·ªÉ so s√°nh (c√≥ th·ªÉ tƒÉng l√™n n·∫øu mu·ªën)
    
    rouge = evaluate.load("rouge")
    final_results = {}

    # --- PH·∫¶N 1: ƒê√ÅNH GI√Å BASE MODEL ---
    print(f"\n--- [1/3] ƒêang ƒë√°nh gi√° BASE MODEL ({model_id}) ---")
    base_model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        quantization_config=bnb_config, 
        device_map="auto",
        trust_remote_code=True,
        dtype=torch.float16
    )
    base_preds, references = run_inference_base_finetuned(base_model, tokenizer, samples)
    final_results["base_model"] = rouge.compute(predictions=base_preds, references=references)
    
    # Gi·∫£i ph√≥ng VRAM c·ªßa Base Model
    del base_model
    torch.cuda.empty_cache()

    # --- PH·∫¶N 2: ƒê√ÅNH GI√Å FINE-TUNED MODEL ---
    print(f"\n--- [2/3] ƒêang ƒë√°nh gi√° FINE-TUNED MODEL (Adapter) ---")
    base_model_for_ft = AutoModelForCausalLM.from_pretrained(
        model_id, 
        quantization_config=bnb_config, 
        device_map="auto",
        trust_remote_code=True,
        dtype=torch.float16
    )
    ft_model = PeftModel.from_pretrained(base_model_for_ft, adapter_path)
    ft_preds, _ = run_inference_base_finetuned(ft_model, tokenizer, samples)
    final_results["finetuned_model"] = rouge.compute(predictions=ft_preds, references=references)
    
    # Gi·∫£i ph√≥ng VRAM c·ªßa Fine-tuned Model
    del ft_model
    del base_model_for_ft
    torch.cuda.empty_cache()

    # --- PH·∫¶N 3: ƒê√ÅNH GI√Å FINE-TUNED + RAG MODEL ---
    print(f"\n--- [3/3] ƒêang ƒë√°nh gi√° FINE-TUNED + RAG MODEL ---")
    # create_rag_system s·∫Ω t·ª± ƒë·ªông load vectorstore n·∫øu ƒë√£ t·ªìn t·∫°i
    rag_chain, _ = create_rag_system(
        model_base_id=model_id,
        adapter_path=adapter_path,
        dataset_name="ruslanmv/ai-medical-chatbot",  # C·∫ßn ƒë·ªÉ t·∫°o m·ªõi n·∫øu ch∆∞a c√≥, nh∆∞ng s·∫Ω kh√¥ng d√πng n·∫øu ƒë√£ c√≥ vectorstore
        vectorstore_path=vectorstore_path
    )
    rag_preds, _ = run_inference_rag(rag_chain, samples)
    final_results["finetuned_rag_model"] = rouge.compute(predictions=rag_preds, references=references)

    # 3. L∆∞u k·∫øt qu·∫£ so s√°nh
    with open(output_metrics, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=4, ensure_ascii=False)
    
    # 4. In b·∫£ng so s√°nh nhanh ra m√†n h√¨nh
    print("\n" + "="*70)
    print(f"{'Metric':<15} | {'Base Model':<15} | {'Fine-tuned':<15} | {'Fine-tuned+RAG':<15}")
    print("-"*70)
    for m in ['rouge1', 'rouge2', 'rougeL']:
        base_v = final_results["base_model"][m]
        ft_v = final_results["finetuned_model"][m]
        rag_v = final_results["finetuned_rag_model"][m]
        print(f"{m:<15} | {base_v:<15.4f} | {ft_v:<15.4f} | {rag_v:<15.4f}")
    print("="*70)
    
    print(f"\n‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_metrics}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--val_file", type=str, default="data/processed/val.jsonl")
    parser.add_argument("--output_metrics", type=str, default="metrics.json")
    parser.add_argument("--model_id", type=str, default="Qwen/Qwen2.5-0.5B-Instruct")
    parser.add_argument("--adapter_path", type=str, default="models/qwen-medical-finetuned/final_adapter")
    parser.add_argument("--vectorstore_path", type=str, default="data/rag_vectorstore")
    args = parser.parse_args()
    
    evaluate_and_compare(args.val_file, args.output_metrics, args.model_id, args.adapter_path, args.vectorstore_path)
